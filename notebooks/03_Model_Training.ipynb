{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d7056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "try:\n",
    "    import gmplot\n",
    "    import googlemaps\n",
    "except ImportError:\n",
    "    %pip install gmplot googlemaps \n",
    "    import gmplot\n",
    "    import googlemaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bfb6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "parent_path = Path().cwd().parent  \n",
    "\n",
    "#.env\n",
    "dotenv_path = parent_path / '.env'\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "GOOGLE_MAPS_API_KEY = os.getenv('GOOGLE_MAPS_API_KEY')\n",
    "\n",
    "#src folder \n",
    "src_path = parent_path / \"src\"\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "# Now import from features package\n",
    "from model.models import run_regression_models\n",
    "from model.models import normalize_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88de586e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../data/processed/feature_engineered_train.csv\",index_col='row_id')\n",
    "test_df = pd.read_csv(\"../data/processed/feature_engineered_test.csv\",index_col='row_id')\n",
    "combine = [train_df,test_df]\n",
    "\n",
    "print(\"Train:\",train_df.shape)\n",
    "print(\"Test:\",test_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725f215d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26143bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Training with Fast Models (for testing)\n",
    "print(\"Training with fast models only...\")\n",
    "fast_models = ['LINREG', 'RIDGE', 'XGB']\n",
    "fast_results = run_regression_models(train_df, fast_models)\n",
    "print(f\"Fast training completed! Trained {len(fast_results)} models.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533edf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Model Training (all models)\n",
    "print(\"Starting full model training...\")\n",
    "print(\"This may take 2-3 minutes for all 7 models...\")\n",
    "\n",
    "models_to_run = ['LINREG','RIDGE','LASSO','SVR','XGB','RF','NN']\n",
    "models = run_regression_models(train_df, models_to_run)\n",
    "print(f\"Full training completed! Trained {len(models)} models.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c51721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Performance Analysis\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model_performance(models_dict, test_data=None):\n",
    "    \"\"\"Evaluate all trained models and create performance summary\"\"\"\n",
    "    \n",
    "    # Prepare validation data\n",
    "    X = train_df.drop(columns=['duration'], axis=1)\n",
    "    Y = train_df['duration']\n",
    "    Xn = normalize_features(X)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=1)\n",
    "    Xn_train, Xn_val, Yn_train, Yn_val = train_test_split(Xn, Y, test_size=0.2, random_state=1)\n",
    "    \n",
    "    performance_results = []\n",
    "    \n",
    "    for model_name, model in models_dict.items():\n",
    "        # Make predictions\n",
    "        if model_name in ['Linear Regression', 'Ridge Regression', 'Lasso Regression', 'Neural Network']:\n",
    "            # Use normalized data for these models\n",
    "            preds = model.predict(Xn_val)\n",
    "            actual = Yn_val\n",
    "        else:\n",
    "            # Use regular data for tree-based models\n",
    "            preds = model.predict(X_val)\n",
    "            actual = Y_val\n",
    "            \n",
    "        # Calculate metrics\n",
    "        rmse = np.sqrt(mean_squared_error(actual, preds))\n",
    "        mae = mean_absolute_error(actual, preds)\n",
    "        r2 = r2_score(actual, preds)\n",
    "        \n",
    "        mask = actual !=0\n",
    "        if mask.sum() > 0:\n",
    "            mape = np.mean(np.abs((actual[mask] - preds[mask]) / actual[mask])) * 100\n",
    "        else:\n",
    "            mape=np.inf\n",
    "        \n",
    "        performance_results.append({\n",
    "            'Model': model_name,\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'R2_Score': r2,\n",
    "            'MAPE': mape\n",
    "        })\n",
    "        \n",
    "        print(f\"{model_name}:\")\n",
    "        print(f\"  RMSE: {rmse:.2f}\")\n",
    "        print(f\"  MAE: {mae:.2f}\")\n",
    "        print(f\"  R¬≤: {r2:.4f}\")\n",
    "        print(f\"  MAPE: {mape:.2f}%\")\n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    return pd.DataFrame(performance_results)\n",
    "\n",
    "# Evaluate model performance\n",
    "print(\"Model Performance Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "performance_df = evaluate_model_performance(models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713c6e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Performance Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Sort by RMSE for better visualization\n",
    "performance_sorted = performance_df.sort_values('RMSE')\n",
    "\n",
    "# RMSE Comparison\n",
    "axes[0,0].barh(performance_sorted['Model'], performance_sorted['RMSE'], color='skyblue')\n",
    "axes[0,0].set_title('Model RMSE Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0,0].set_xlabel('RMSE')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# MAE Comparison\n",
    "axes[0,1].barh(performance_sorted['Model'], performance_sorted['MAE'], color='lightcoral')\n",
    "axes[0,1].set_title('Model MAE Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0,1].set_xlabel('MAE')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# R¬≤ Score Comparison\n",
    "axes[1,0].barh(performance_sorted['Model'], performance_sorted['R2_Score'], color='lightgreen')\n",
    "axes[1,0].set_title('Model R¬≤ Score Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1,0].set_xlabel('R¬≤ Score')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# MAPE Comparison\n",
    "axes[1,1].barh(performance_sorted['Model'], performance_sorted['MAPE'], color='gold')\n",
    "axes[1,1].set_title('Model MAPE Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1,1].set_xlabel('MAPE (%)')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print best performing model\n",
    "best_model = performance_sorted.iloc[0]\n",
    "print(f\"\\nüèÜ Best Performing Model: {best_model['Model']}\")\n",
    "print(f\"   RMSE: {best_model['RMSE']:.2f}\")\n",
    "print(f\"   R¬≤ Score: {best_model['R2_Score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f6e876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance Analysis (for tree-based models)\n",
    "from model.models import plot_feature_importance\n",
    "\n",
    "# Get feature importance for tree-based models\n",
    "tree_models = ['XGBoost', 'Random Forest']\n",
    "\n",
    "for model_name in tree_models:\n",
    "    if model_name in models:\n",
    "        print(f\"\\nFeature Importance for {model_name}:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Prepare data for feature importance\n",
    "        X = train_df.drop(columns=['duration'], axis=1)\n",
    "        model = models[model_name]\n",
    "        \n",
    "        # Plot feature importance\n",
    "        plot_feature_importance(model, X)\n",
    "        plt.title(f'{model_name} - Feature Importance', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598761d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning for XGBoost\n",
    "from model.models import hyperparameter_tuning_xgb\n",
    "\n",
    "print(\"Starting XGBoost Hyperparameter Tuning...\")\n",
    "print(\"This will test different parameter combinations...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "best_xgb, best_params, best_rmse = hyperparameter_tuning_xgb(train_df)\n",
    "\n",
    "print(f\"\\nüéØ Hyperparameter Tuning Results:\")\n",
    "print(f\"Best RMSE: {best_rmse:.4f}\")\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "# Add tuned model to our models dictionary\n",
    "models['XGBoost_Tuned'] = best_xgb\n",
    "print(f\"\\n‚úÖ Tuned XGBoost model added to models dictionary!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522b395c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Predictions on Test Data\n",
    "from model.models import predict_duration, to_submission\n",
    "\n",
    "print(\"Making predictions on test data...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Make predictions with best model\n",
    "best_model_name = performance_df.loc[performance_df['RMSE'].idxmin(), 'Model']\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "print(f\"Using best model: {best_model_name}\")\n",
    "\n",
    "# Make predictions\n",
    "test_predictions = predict_duration(best_model, test_df, best_model_name)\n",
    "\n",
    "# Create submission file\n",
    "submission_file = to_submission(test_predictions, \"output/notebook_submission\")\n",
    "print(f\"\\nüìÑ Submission file created: {submission_file}\")\n",
    "\n",
    "# Show prediction statistics\n",
    "print(f\"\\nüìä Prediction Statistics:\")\n",
    "print(f\"Min prediction: {test_predictions.min():.2f}\")\n",
    "print(f\"Max prediction: {test_predictions.max():.2f}\")\n",
    "print(f\"Mean prediction: {test_predictions.mean():.2f}\")\n",
    "print(f\"Std prediction: {test_predictions.std():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2809da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Comparison with Predictions\n",
    "from model.models import compare_predictions\n",
    "\n",
    "print(\"Comparing model predictions...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Compare predictions from different models\n",
    "if len(models) >= 2:\n",
    "    model_names = list(models.keys())\n",
    "    \n",
    "    # Compare best model with second best\n",
    "    if len(model_names) >= 2:\n",
    "        model1_name = performance_df.loc[performance_df['RMSE'].idxmin(), 'Model']\n",
    "        model2_name = performance_df.loc[performance_df['RMSE'].nsmallest(2).index[1], 'Model']\n",
    "        \n",
    "        # Make predictions for comparison\n",
    "        pred1 = predict_duration(models[model1_name], test_df, model1_name)\n",
    "        pred2 = predict_duration(models[model2_name], test_df, model2_name)\n",
    "        \n",
    "        # Compare predictions\n",
    "        compare_predictions(\n",
    "            pred1, pred2,\n",
    "            title=f\"{model1_name} vs {model2_name} Predictions\"\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Comparison completed between {model1_name} and {model2_name}\")\n",
    "else:\n",
    "    print(\"Need at least 2 models for comparison\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f86046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Models and Results\n",
    "from model.save_models import save_model, save_model_results\n",
    "import json\n",
    "\n",
    "print(\"Saving models and results...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Save all trained models\n",
    "saved_models = {}\n",
    "for model_name, model in models.items():\n",
    "    try:\n",
    "        model_path = save_model(\n",
    "            model=model,\n",
    "            model_name=model_name.replace(' ', '_').lower(),\n",
    "            output_dir='../saved_models'\n",
    "        )\n",
    "        saved_models[model_name] = model_path\n",
    "        print(f\"‚úÖ Saved {model_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to save {model_name}: {e}\")\n",
    "\n",
    "# Save performance results\n",
    "performance_results = {\n",
    "    'performance_metrics': performance_df.to_dict('records'),\n",
    "    'best_model': performance_df.loc[performance_df['RMSE'].idxmin(), 'Model'],\n",
    "    'best_rmse': performance_df['RMSE'].min(),\n",
    "    'training_timestamp': pd.Timestamp.now().isoformat()\n",
    "}\n",
    "\n",
    "results_path = save_model_results(performance_results, '../saved_models')\n",
    "print(f\"\\nüìä Performance results saved: {results_path}\")\n",
    "\n",
    "print(f\"\\nüéâ All models and results saved successfully!\")\n",
    "print(f\"Total models saved: {len(saved_models)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a8eb44",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### What We've Accomplished:\n",
    "1. **Trained 7 different ML models** for trip duration prediction\n",
    "2. **Evaluated model performance** using multiple metrics (RMSE, MAE, R¬≤, MAPE)\n",
    "3. **Visualized model comparisons** with comprehensive charts\n",
    "4. **Analyzed feature importance** for tree-based models\n",
    "5. **Performed hyperparameter tuning** for XGBoost\n",
    "6. **Generated predictions** on test data\n",
    "7. **Created submission files** ready for competition\n",
    "8. **Saved all models and results** for future use\n",
    "\n",
    "### Key Insights:\n",
    "- **Best performing model**: [Will be shown after running the cells above]\n",
    "- **Feature importance**: [Will be displayed in feature importance plots]\n",
    "- **Model performance range**: [Will be shown in performance analysis]\n",
    "\n",
    "### Next Steps:\n",
    "1. **Submit predictions** using the generated CSV files\n",
    "2. **Experiment with ensemble methods** combining multiple models\n",
    "3. **Feature engineering** based on feature importance analysis\n",
    "4. **Cross-validation** for more robust model evaluation\n",
    "5. **Model deployment** for real-time predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d914b038",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
